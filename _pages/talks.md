---
layout: page
title: talks
permalink: /talks/
nav: true
nav_order: 3
description: Conference presentations, seminars, and workshop talks on optimization, machine learning, and LLM research.

selected_papers: false
social: false

latest_posts:
  enabled: false
---

## Invited Seminars


**Morgan Stanley Machine Learning Research** *(December 2025)*
*What is the lon-run behaviour of SGD* — New York, USA
Materials: [slides](/assets/pdf/slides_MS_Dec_2025.pdf)

**Inria Argo Team Seminar** *(December 2025)*
*What is the lon-run behaviour of SGD* — Grenoble, France
Materials: [slides](/assets/pdf/slides_Argo_Dec_2025.pdf)


**Université de Nice** *(December 2024)*
*What is the long-run distribution of SGD?* — Nice, France
Materials: [seminar page](https://math.univ-cotedazur.fr/laboratoire/seminaires/séminaires-de-léquipe-probabilités-statistiques) • [slides](/assets/pdf/slides_UNice_Dec_2024.pdf)

**LPSM, Université de Paris** *(October 2024)*
*What is the long-run distribution of SGD?* — Paris, France
Materials: [seminar page](https://www.lpsm.paris/seminaires/statp6p7/index) • [slides](/assets/pdf/slides_UParis_Oct_2024.pdf)

**Thoth Team Seminar, Inria** *(October 2024)*
*What is the long-run distribution of SGD?* — Grenoble, France
Materials: [seminar page](https://team.inria.fr/thoth/) • [slides](/assets/pdf/slides_retraite_thoth_2024.pdf)

**SMAI MODE Conference** *(April 2024)*
*Mirror methods: deterministic analysis* — Marseille, France
Materials: [slides](/assets/pdf/slides_smai_mode2024.pdf)


**Thoth Team Seminar, Inria** *(April 2021)*
*Expressive power of invariant and equivariant graph neural networks* — Grenoble, France
Materials: [slides](/assets/pdf/slides_gnn.pdf)

## Conference and Workshop Presentations


**ICML 2025** — *Upcoming*
*The global convergence time of stochastic gradient descent in non-convex landscapes*
Materials: [poster](/assets/pdf/poster_icml25.pdf) • [paper](https://arxiv.org/abs/2503.16398)


**ICML 2024** — *Montreal, Canada*
*What is the long-run distribution of stochastic gradient descent? A large deviations analysis*
Materials: [poster](/assets/pdf/poster_icml24.pdf) • [paper](https://arxiv.org/abs/2406.09241)


**NeurIPS@Paris 2023** — *Paris, France*
*Exact generalization guarantees for (regularized) Wasserstein distributionally robust models*
Materials: [workshop page](https://neuripsinparis.github.io/neurips2023paris/) • [slides](/assets/pdf/slides_neurips_in_paris_2023.pdf) • [paper](https://arxiv.org/abs/2305.17076)

**FOCM 2023** — *Paris, France*
*Exact generalization guarantees for (regularized) Wasserstein distributionally robust models*
Materials: [conference page](https://focm2023.org/) • [poster](/assets/pdf/poster_wdro.pdf) • [paper](https://arxiv.org/abs/2305.17076)


**ICCOPT 2022** — *Bethlehem, USA*
*Mirror methods in stochastic settings*
Materials: [slides](/assets/pdf/slides_iccopt.pdf)

**Workshop in Erice 2022** — *Sicily, Italy*
*Regularized WDRO and generalization guarantees*
Materials: [workshop page](https://workshopsperice2022.github.io/) • [slides](/assets/pdf/slides_sicile.pdf)

**COLT 2021** — *Boulder, USA*
*The last-iterate convergence rate of optimistic mirror descent in stochastic variational inequalities*
Materials: [slides](/assets/pdf/slides_colt.pdf) • [poster](/assets/pdf/poster_colt.pdf) • [paper](https://arxiv.org/abs/2107.01906)

**MIPT-UGA Workshop 2021** — *Online*
*Expressive power of invariant and equivariant graph neural networks*
Materials: [workshop page](https://sites.google.com/view/mipt-uga-ai-workshop/home) • [slides](/assets/pdf/slides_gnn.pdf) • [paper](https://arxiv.org/abs/2006.15646)

**AISTATS 2020** — *Online*
*A tight and unified analysis of gradient-based methods for a whole spectrum of differentiable games*
Materials: [slides](/assets/pdf/slides_aistats.pdf) • [paper](https://arxiv.org/abs/1906.05945)


