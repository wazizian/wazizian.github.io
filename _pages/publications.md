---
layout: page
permalink: /publications/
title: publications
description: Research contributions towards a principled understanding of optimization dynamics in deep learning.
nav: true
nav_order: 2
---

## Stochastic optimization in deep learning

We study the long-run behavior of stochastic gradient descent (SGD) on non-convex objectives, providing the first characterization of SGD's invariant measures and global convergence times.

- **What is the long-run distribution of stochastic gradient descent?** ([ICML 2024](https://arxiv.org/abs/2406.09241), [poster](/assets/pdf/poster_icml24.pdf))
- **The global convergence time of stochastic gradient descent in non-convex landscapes** ([ICML 2025](https://arxiv.org/abs/2503.16398), [poster](/assets/pdf/poster_icml25.pdf))

**Talks:** Thoth seminar ([slides](/assets/pdf/slides_retraite_thoth_2024.pdf)), LPSM Paris ([slides](/assets/pdf/slides_UParis_Oct_2024.pdf)), Université Côte d'Azur ([slides](/assets/pdf/slides_UNice_Dec_2024.pdf)), Morgan Stanley ML Research ([slides](/assets/pdf/slides_MS_Dec_2025.pdf)), Inria Argo team ([slides](/assets/pdf/slides_Argo_Dec_2025.pdf))

## Internal mechanisms of large language models

Understanding the robustness of uncertainty quantification methods and in-context learning capabilities through targeted experiments.

- **The geometries of truth are orthogonal across tasks** ([R2FM Workshop@ICML 2025](https://arxiv.org/pdf/2506.08572)) — Work at [Apple ML Research](https://machinelearning.apple.com/)
- **How does the pretraining distribution shape in-context learning?** ([arXiv](https://arxiv.org/abs/2510.01163)) — Work at [Morgan Stanley ML Research](https://www.morganstanley.com/about-us/technology/machine-learning-research-team)

## Wasserstein distributionally robust optimization

Regularization schemes and generalization guarantees for Wasserstein DRO models.

- **Regularization for Wasserstein distributionally robust optimization** ([ESAIM COCV](https://arxiv.org/abs/2205.08826))
- **Exact generalization guarantees for (regularized) Wasserstein distributionally robust models** ([NeurIPS 23](https://arxiv.org/abs/2305.17076), [slides](/assets/pdf/slides_neurips_2023.pdf))

**Talks:** Erice 2022 ([slides](/assets/pdf/slides_sicile.pdf)), FOCM 2023 ([poster](/assets/pdf/poster_wdro.pdf)), NeurIPS@Paris 2023 ([slides](/assets/pdf/slides_neurips_in_paris_2023.pdf))

## Last-iterate convergence of mirror methods

Determining how Bregman geometry impacts last-iterate guarantees in variational inequalities.

- **The last-iterate convergence rate of optimistic mirror descent in stochastic variational inequalities** ([COLT 21](https://arxiv.org/abs/2107.01906), [slides](/assets/pdf/slides_colt.pdf), [poster](/assets/pdf/poster_colt.pdf))
- **The rate of convergence of Bregman proximal methods** ([to be published in SIOPT](https://arxiv.org/abs/2211.08043))

**Talks:** COLT 21, ICCOPT 22 ([slides](/assets/pdf/slides_iccopt.pdf)), SMAI MODE 2024 ([slides](/assets/pdf/slides_smai_mode2024.pdf))

## Graph neural networks

- **Expressive power of invariant and equivariant graph neural networks** ([ICLR 21](https://arxiv.org/abs/2006.15646), [slides](/assets/pdf/slides_gnn.pdf)) — with [Marc Lelarge](https://www.di.ens.fr/~lelarge/)

## Smooth game optimization for Machine Learning

Unified analyses and accelerated methods for differentiable games.

- **A tight and unified analysis of gradient-based methods for a whole spectrum of differentiable games** ([AISTATS 20](https://arxiv.org/abs/1906.05945), [slides](/assets/pdf/slides_aistats.pdf))
- **Accelerating smooth games by manipulating spectral shapes** ([AISTATS 20](https://arxiv.org/abs/2001.00602))

## Full bibliography

{% include bib_search.liquid %}

<div class="publications">

{% bibliography %}

</div>
