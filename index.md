---
layout: default
---
![Banner](assets/picture.jpg){: width="1500"}

<!-- Welcome to [Waïss Azizian](https://wazizian.fr)'s personal website! -->

### About
I am a PhD student in applied mathematics in Grenoble. I have the honour of having the amazing trio [Franck Iutzeler](https://www.iutzeler.org/), [Jérôme Malick](https://membres-ljk.imag.fr/Jerome.Malick/) and [Panayotis Mertikopoulos](https://polaris.imag.fr/panayotis.mertikopoulos/) as adviors. More precisely, I am at the [LJK lab](https://www-ljk.imag.fr/), which is part of [UGA](https://www.univ-grenoble-alpes.fr/).

### Contact
- Email: waiss (dot) azizian (at) univ-grenoble-alpes (dot) fr
- Physical: office 143, LJK lab, IMAG building.

I am in charge of the [team's seminar](https://sites.google.com/view/gorgeous-optim/), please get in touch if you would like to present!



### Research
My current interest are robust optimization, non-convex stochastic optimization and online learning.

See [arXiv](https://arxiv.org/a/azizian_w_1.html), [Google Scholar](https://scholar.google.fr/citations?user=oXxTTe8AAAAJ&hl=fr), [DBLP](https://dblp.org/pid/243/3135.html) and [Github](https://github.com/wazizian).

#### Wasserstein Distributionnally Robust Optimization
 Inspired by the success of entropic regularization in optimal transport, we study the regularization of WDRO ([ESAIM COCV](https://arxiv.org/abs/2205.08826)).
 We also show that these estimators enjoy attractive generalization guarantees ([NeurIPS 23](https://arxiv.org/abs/2305.17076)).

I presented early versions of these works at [a workshop in Erice in May 2022](https://workshopsperice2022.github.io/), ([slides](pdf/slides_sicile.pdf)), and the second part at [FOCM 2023](https://focm2023.org/) in Paris, ([poster](pdf/poster_wdro.pdf)).

#### Last-iterate convergence of mirror methods
We characterize the last iterate convergence rate of mirror methods in variational inequalities as a function of the local geometry of the Bregman divergence near the solution, both in the deterministic ([under review](https://arxiv.org/abs/2211.08043)) and stochastic settings ([COLT 21](https://arxiv.org/abs/2107.01906)).

The latter was presented at COLT 21 ([slides](pdf/slides_colt.pdf), [poster](pdf/poster_colt.pdf)) and at ICCOPT 22 ([slides](pdf/slides_iccopt.pdf)).

#### Graph Neural Networks
With [Marc Lelarge](https://www.di.ens.fr/~lelarge/), we precisely describe the approximatyion cabapilities of invariant and equivariant graph neural networks ([ICLR 21](https://arxiv.org/abs/2006.15646)). It was presented at a [MIPT-UGA workshop](https://sites.google.com/view/mipt-uga-ai-workshop/home) and at the Thoth team seminar ([slides](pdf/slides_gnn.pdf)).

#### Smooth game optimization for Machine Learning
With [Gauthier Gidel](https://gauthiergidel.github.io/), [Ioannis Mitliagkas](https://mitliagkas.github.io/) and [Simon Lacoste-Julien](https://www.iro.umontreal.ca/~slacoste/), we propose a tight and unified analysis of gradient-based methods in games ([AISTATS 20](https://arxiv.org/abs/1906.05945), [slides](pdf/slides_aistats.pdf)) and leverage matrix iteration theory to study accelerated methods in games ([AISTATS 20](https://arxiv.org/abs/2001.00602)).


### Teaching
#### 2023-2024
- Statistics for biology (second year of bachelor): exercise sessions.

#### 2022-2023
- Numerical optimization (second year at ENSIMAG): practical sessions in Python/Numpy and exercise sessions.
- Numerical optimization (first year master MSIAM): practical sessions in Python/Numpy.
- Numerical analysis (first year ENSIMAG): practical sessions in Python/Numpy.
