---
layout: page
title: Stochastic optimization in deep learning
description: Large-deviation analysis of SGD invariant measures and global convergence times.
importance: 1
category: research
---

We study how stochastic gradient descent behaves asymptotically on non-convex objectives. Two complementary angles drive this project:

1. **Invariant measure of SGD.** Using tools from large deviations, we describe the stationary distribution of SGD iterates on general non-convex landscapes, leading to the ICML 2024 paper *What is the long-run distribution of stochastic gradient descent?* ([ICML 2024](https://arxiv.org/abs/2406.09241), [poster](/assets/pdf/poster_icml24.pdf)).
2. **Global convergence time.** We quantify the time it takes for SGD to discover global minima, highlighting how the loss geometry and noise structure interact ([ICML 2025](https://arxiv.org/abs/2503.16398), [poster](/assets/pdf/poster_icml25.pdf)).

These results have been presented at the Thoth seminar ([slides](/assets/pdf/slides_retraite_thoth_2024.pdf)), the LPSM statistics seminar ([slides](/assets/pdf/slides_UParis_Oct_2024.pdf)), Université Côte d'Azur ([slides](/assets/pdf/slides_UNice_Dec_2024.pdf)), Morgan Stanley's ML Research seminar ([slides](/assets/pdf/slides_MS_Dec_2025.pdf)), and Inria's Argo team ([slides](/assets/pdf/slides_Argo_Dec_2025.pdf)).
